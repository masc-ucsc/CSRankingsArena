{
  "papers": [
    {
      "id": "2305.10403",
      "arxivId": "2305.10403",
      "title": "Real-Time Neural Radiance Fields for Autonomous Driving Simulation",
      "authors": ["Jason Zhang", "Sara Johnson", "Michael Chen"],
      "abstract": "Neural radiance fields (NeRF) have emerged as a powerful representation for 3D scenes, enabling high-quality novel view synthesis. In this paper, we present a real-time NeRF system specifically designed for autonomous driving simulation. Our approach leverages temporal coherence and scene understanding to achieve a 10x speedup over traditional NeRF rendering while maintaining visual fidelity. We demonstrate our system's ability to generate photorealistic sensor data for autonomous vehicle testing across diverse weather conditions and environments.",
      "categories": ["cs.CV", "cs.AI", "cs.RO"],
      "subcategories": ["vision", "robotics"],
      "published": "2023-05-17T12:30:45Z",
      "updated": "2023-05-20T09:15:22Z",
      "url": "https://arxiv.org/abs/2305.10403",
      "pdfUrl": "https://arxiv.org/pdf/2305.10403.pdf",
      "publishedYear": 2023
    },
    {
      "id": "2304.12372",
      "arxivId": "2304.12372",
      "title": "Efficient Transformers with Linear Attention Mechanisms",
      "authors": ["Emily Rodriguez", "David Wilson", "Jessica Park"],
      "abstract": "Transformer models have revolutionized machine learning across domains, but their quadratic attention mechanism limits scalability to long sequences. In this work, we introduce a novel linear attention mechanism that reduces the computational complexity from O(nÂ²) to O(n) while preserving model performance. Our method uses a kernel-based approximation that enables processing of sequences up to 100,000 tokens on consumer hardware. Experiments on language modeling, image recognition, and genomic sequence analysis demonstrate that our approach achieves comparable accuracy to standard transformers while offering significant speedups and memory efficiency.",
      "categories": ["cs.LG", "cs.AI", "cs.CL"],
      "subcategories": ["ml", "llm"],
      "published": "2023-04-24T15:42:18Z",
      "updated": "2023-04-26T11:20:37Z",
      "url": "https://arxiv.org/abs/2304.12372",
      "pdfUrl": "https://arxiv.org/pdf/2304.12372.pdf",
      "publishedYear": 2023
    },
    {
      "id": "2303.08774",
      "arxivId": "2303.08774",
      "title": "BioLLM: A Domain-Specific Large Language Model for Biomedical Text Processing",
      "authors": ["Alexander Lee", "Priya Patel", "Robert Thompson", "Laura Garcia"],
      "abstract": "Large language models (LLMs) have shown impressive capabilities across diverse tasks, but often lack specialized knowledge for domain-specific applications. We present BioLLM, a large language model fine-tuned specifically for biomedical text processing. Starting from a 7B parameter base model, we further train on a curated corpus of 2.3M biomedical papers and 15M clinical notes. Our model demonstrates superior performance on medical knowledge retrieval, clinical reasoning, and biomedical entity extraction compared to general-purpose LLMs of similar size. We also introduce novel techniques for incorporating structured knowledge from databases like UMLS and PubChem during inference without additional fine-tuning.",
      "categories": ["cs.CL", "cs.LG", "q-bio.QM"],
      "subcategories": ["nlp", "llm"],
      "published": "2023-03-15T18:23:59Z",
      "updated": "2023-03-18T14:05:31Z",
      "url": "https://arxiv.org/abs/2303.08774",
      "pdfUrl": "https://arxiv.org/pdf/2303.08774.pdf",
      "publishedYear": 2023
    }
  ],
  "pagination": {
    "total": 27,
    "page": 1,
    "limit": 20,
    "pages": 2
  }
}