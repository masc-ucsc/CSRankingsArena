[
  {
    "id": "2305.10403",
    "arxivId": "2305.10403",
    "title": "Real-Time Neural Radiance Fields for Autonomous Driving Simulation",
    "authors": ["Jason Zhang", "Sara Johnson", "Michael Chen"],
    "abstract": "Neural radiance fields (NeRF) have emerged as a powerful representation for 3D scenes, enabling high-quality novel view synthesis. In this paper, we present a real-time NeRF system specifically designed for autonomous driving simulation. Our approach leverages temporal coherence and scene understanding to achieve a 10x speedup over traditional NeRF rendering while maintaining visual fidelity. We demonstrate our system's ability to generate photorealistic sensor data for autonomous vehicle testing across diverse weather conditions and environments.",
    "categories": ["cs.CV", "cs.AI", "cs.RO"],
    "subcategories": ["vision", "robotics"],
    "published": "2023-05-17T12:30:45Z",
    "updated": "2023-05-20T09:15:22Z",
    "url": "https://arxiv.org/abs/2305.10403",
    "pdfUrl": "https://arxiv.org/pdf/2305.10403.pdf",
    "publishedYear": 2023
  },
  {
    "id": "2304.12372",
    "arxivId": "2304.12372",
    "title": "Efficient Transformers with Linear Attention Mechanisms",
    "authors": ["Emily Rodriguez", "David Wilson", "Jessica Park"],
    "abstract": "Transformer models have revolutionized machine learning across domains, but their quadratic attention mechanism limits scalability to long sequences. In this work, we introduce a novel linear attention mechanism that reduces the computational complexity from O(nÂ²) to O(n) while preserving model performance. Our method uses a kernel-based approximation that enables processing of sequences up to 100,000 tokens on consumer hardware. Experiments on language modeling, image recognition, and genomic sequence analysis demonstrate that our approach achieves comparable accuracy to standard transformers while offering significant speedups and memory efficiency.",
    "categories": ["cs.LG", "cs.AI", "cs.CL"],
    "subcategories": ["ml", "llm"],
    "published": "2023-04-24T15:42:18Z",
    "updated": "2023-04-26T11:20:37Z",
    "url": "https://arxiv.org/abs/2304.12372",
    "pdfUrl": "https://arxiv.org/pdf/2304.12372.pdf",
    "publishedYear": 2023
  },
  {
    "id": "2303.08774",
    "arxivId": "2303.08774",
    "title": "BioLLM: A Domain-Specific Large Language Model for Biomedical Text Processing",
    "authors": ["Alexander Lee", "Priya Patel", "Robert Thompson", "Laura Garcia"],
    "abstract": "Large language models (LLMs) have shown impressive capabilities across diverse tasks, but often lack specialized knowledge for domain-specific applications. We present BioLLM, a large language model fine-tuned specifically for biomedical text processing. Starting from a 7B parameter base model, we further train on a curated corpus of 2.3M biomedical papers and 15M clinical notes. Our model demonstrates superior performance on medical knowledge retrieval, clinical reasoning, and biomedical entity extraction compared to general-purpose LLMs of similar size. We also introduce novel techniques for incorporating structured knowledge from databases like UMLS and PubChem during inference without additional fine-tuning.",
    "categories": ["cs.CL", "cs.LG", "q-bio.QM"],
    "subcategories": ["nlp", "llm"],
    "published": "2023-03-15T18:23:59Z",
    "updated": "2023-03-18T14:05:31Z",
    "url": "https://arxiv.org/abs/2303.08774",
    "pdfUrl": "https://arxiv.org/pdf/2303.08774.pdf",
    "publishedYear": 2023
  },
  {
    "id": "2307.01234",
    "arxivId": "2307.01234",
    "title": "SysGen: An LLM-based System Design Generator for Custom SoCs",
    "authors": ["Thomas Anderson", "Jennifer Kim", "Richard Davis"],
    "abstract": "Designing application-specific System-on-Chips (SoCs) requires significant expertise and effort. We present SysGen, an LLM-based tool that automatically generates complete system designs from high-level requirements. Our approach uses a specialized architecture language model to translate natural language specifications into optimized hardware block diagrams, interconnect topologies, and memory hierarchies. SysGen incorporates domain-specific constraints like power, performance, and area targets while ensuring design correctness through formal verification methods. Evaluation on 50 diverse design scenarios shows that SysGen produces implementable designs that match or exceed expert-created solutions in 78% of cases while reducing design time from weeks to hours.",
    "categories": ["cs.AR", "cs.AI"],
    "subcategories": ["processors", "accelerators"],
    "published": "2023-07-03T09:45:12Z",
    "updated": "2023-07-10T16:20:43Z",
    "url": "https://arxiv.org/abs/2307.01234",
    "pdfUrl": "https://arxiv.org/pdf/2307.01234.pdf",
    "publishedYear": 2023
  },
  {
    "id": "2302.05642",
    "arxivId": "2302.05642",
    "title": "QuantUM: Quantization of Unified Memory for Efficient Multi-Accelerator Training",
    "authors": ["William Zhang", "Sarah Brown", "Daniel Robinson"],
    "abstract": "Training modern deep learning models often requires multiple accelerators working in parallel, with significant memory transfers between devices. We introduce QuantUM, a novel method that applies dynamic quantization to data stored in unified memory systems. Our approach adaptively determines optimal precision for different tensor regions based on access patterns and gradient sensitivity, reducing memory bandwidth requirements by up to 4x with negligible impact on training accuracy. QuantUM is implemented as a transparent middleware layer that requires no changes to existing ML frameworks or models. Experiments on vision, language, and recommendation model training show 35-65% performance improvements on multi-GPU and GPU-CPU hybrid systems.",
    "categories": ["cs.AR", "cs.DC", "cs.LG"],
    "subcategories": ["memory", "accelerators"],
    "published": "2023-02-11T10:15:33Z",
    "updated": "2023-02-15T08:42:19Z",
    "url": "https://arxiv.org/abs/2302.05642",
    "pdfUrl": "https://arxiv.org/pdf/2302.05642.pdf",
    "publishedYear": 2023
  },
  {
    "id": "2306.12984",
    "arxivId": "2306.12984",
    "title": "Hypervisor-Level Disaggregation for Cloud-Native Memory Management",
    "authors": ["Carlos Martinez", "Lisa Wang", "James Smith"],
    "abstract": "Modern cloud workloads have increasingly diverse memory requirements that traditional virtualization struggles to accommodate efficiently. We present a hypervisor-level memory disaggregation architecture that enables fine-grained, application-aware memory management across datacenter resources. Our system introduces a novel two-tier paging mechanism that maintains NUMA-awareness while allowing memory to be elastically allocated across machine boundaries with minimal performance overhead. Through careful optimization of page migration policies and remote memory access protocols, we achieve near-local memory performance for disaggregated memory while enabling 30-45% higher datacenter memory utilization. Evaluation on production-scale workloads shows an average performance overhead of only 3-7% compared to local memory, while enabling much more flexible resource allocation.",
    "categories": ["cs.OS", "cs.DC"],
    "subcategories": ["virtualization", "distributed"],
    "published": "2023-06-22T16:18:29Z",
    "updated": "2023-06-25T11:33:47Z",
    "url": "https://arxiv.org/abs/2306.12984",
    "pdfUrl": "https://arxiv.org/pdf/2306.12984.pdf",
    "publishedYear": 2023
  },
  {
    "id": "2301.03778",
    "arxivId": "2301.03778",
    "title": "SchedOS: A Lightweight Kernel for Mixed-Criticality Real-Time Systems",
    "authors": ["Natalia Rodriguez", "Kevin Murphy", "Hiroshi Tanaka"],
    "abstract": "Mixed-criticality systems must simultaneously support hard real-time guarantees for safety-critical tasks alongside best-effort execution for non-critical workloads. We present SchedOS, a lightweight kernel architecture specifically designed for these requirements. SchedOS features a novel compositional scheduling framework that provides temporal isolation between criticality domains while maximizing CPU utilization. Our formal verification approach guarantees worst-case execution time bounds for critical tasks, even under peak load conditions or in the presence of compromised non-critical components. Evaluation on automotive and aerospace benchmark suites demonstrates SchedOS's ability to maintain hard deadlines for critical tasks while achieving 15-25% higher throughput for non-critical workloads compared to existing mixed-criticality systems.",
    "categories": ["cs.OS", "cs.CR"],
    "subcategories": ["kernel", "real-time"],
    "published": "2023-01-09T14:27:36Z",
    "updated": "2023-01-12T09:18:54Z",
    "url": "https://arxiv.org/abs/2301.03778",
    "pdfUrl": "https://arxiv.org/pdf/2301.03778.pdf",
    "publishedYear": 2023
  },
  {
    "id": "2309.15462",
    "arxivId": "2309.15462",
    "title": "SecureNet: Post-Quantum Protocols for Zero-Trust Network Architecture",
    "authors": ["Michelle Chen", "Ahmed Hassan", "Sophia Williams"],
    "abstract": "With the advancement of quantum computing, existing network security protocols face unprecedented threats. We introduce SecureNet, a comprehensive network security architecture designed to be resistant to quantum attacks while maintaining compatibility with existing infrastructure. Our framework integrates lattice-based cryptographic primitives into standard network protocols, including a novel quantum-resistant VPN protocol and authenticated routing mechanisms. SecureNet adopts a zero-trust architecture that verifies every connection attempt regardless of source or destination. Performance evaluation on both simulated and real-world networks demonstrates that SecureNet adds only 5-12% overhead compared to classical security protocols while providing protection against both current and quantum adversaries.",
    "categories": ["cs.CR", "cs.NI"],
    "subcategories": ["network-security", "crypto"],
    "published": "2023-09-26T08:12:45Z",
    "updated": "2023-09-30T15:20:18Z",
    "url": "https://arxiv.org/abs/2309.15462",
    "pdfUrl": "https://arxiv.org/pdf/2309.15462.pdf",
    "publishedYear": 2023
  },
  {
    "id": "2210.09876",
    "arxivId": "2210.09876",
    "title": "TransWireless: Transformer-based 6G Protocol Design with Reinforcement Learning",
    "authors": ["Alex Johnson", "Maria Garcia", "Toshiro Yamamoto"],
    "abstract": "Next-generation 6G wireless networks require protocols that can adapt to diverse and dynamic environments. We present TransWireless, a transformer-based approach for automatically designing wireless protocols using reinforcement learning. Our framework encodes channel conditions, traffic patterns, and latency requirements into a sequence representation that transformer models can process to generate optimized protocol parameters. TransWireless continuously learns from network behavior, adapting parameters like modulation schemes, frame structures, and retransmission policies to maximize spectral efficiency and energy conservation. Evaluation in diverse wireless environments shows that TransWireless protocols achieve 40-60% better spectral efficiency and 30% lower energy consumption compared to fixed 5G NR protocols while maintaining reliability targets.",
    "categories": ["cs.NI", "cs.LG", "eess.SP"],
    "subcategories": ["wireless", "protocols"],
    "published": "2022-10-18T11:42:38Z",
    "updated": "2022-11-05T14:26:59Z",
    "url": "https://arxiv.org/abs/2210.09876",
    "pdfUrl": "https://arxiv.org/pdf/2210.09876.pdf",
    "publishedYear": 2022
  },
  {
    "id": "2311.27539",
    "arxivId": "2311.27539",
    "title": "PrivacyLens: Automated Analysis of Privacy Policies using Large Language Models",
    "authors": ["Samantha Taylor", "Brian Wilson", "Neha Sharma", "Marco Rossi"],
    "abstract": "Privacy policies are often lengthy and difficult for users to understand, yet they govern how organizations collect and use personal data. We present PrivacyLens, a system that automatically analyzes privacy policies using large language models to extract, categorize, and explain privacy practices in clear language. Our approach combines prompt engineering with fine-tuning on a dataset of annotated privacy policies to achieve high accuracy in identifying data collection practices, sharing arrangements, user rights, and compliance status. PrivacyLens can generate simplified summaries, comparative analyses between different policies, and highlight potential privacy concerns. Evaluation across 500 privacy policies shows that PrivacyLens achieves 91% accuracy in extracting critical privacy statements compared to expert human analysis, while providing explanations that significantly improve user comprehension of privacy implications.",
    "categories": ["cs.CR", "cs.CL", "cs.HC"],
    "subcategories": ["privacy", "nlp"],
    "published": "2023-11-15T16:28:54Z",
    "updated": "2023-11-18T09:32:17Z",
    "url": "https://arxiv.org/abs/2311.27539",
    "pdfUrl": "https://arxiv.org/pdf/2311.27539.pdf",
    "publishedYear": 2023
  }
]