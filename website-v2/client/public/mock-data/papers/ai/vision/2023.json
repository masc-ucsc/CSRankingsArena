[
  {
    "id": "2305.10403",
    "arxivId": "2305.10403",
    "title": "Real-Time Neural Radiance Fields for Autonomous Driving Simulation",
    "authors": ["Jason Zhang", "Sara Johnson", "Michael Chen"],
    "abstract": "Neural radiance fields (NeRF) have emerged as a powerful representation for 3D scenes, enabling high-quality novel view synthesis. In this paper, we present a real-time NeRF system specifically designed for autonomous driving simulation. Our approach leverages temporal coherence and scene understanding to achieve a 10x speedup over traditional NeRF rendering while maintaining visual fidelity. We demonstrate our system's ability to generate photorealistic sensor data for autonomous vehicle testing across diverse weather conditions and environments.",
    "categories": ["cs.CV", "cs.AI", "cs.RO"],
    "subcategories": ["vision", "robotics"],
    "published": "2023-05-17T12:30:45Z",
    "updated": "2023-05-20T09:15:22Z",
    "url": "https://arxiv.org/abs/2305.10403",
    "pdfUrl": "https://arxiv.org/pdf/2305.10403.pdf",
    "publishedYear": 2023
  },
  {
    "id": "2303.14158",
    "arxivId": "2303.14158",
    "title": "Self-Supervised Depth Estimation from Monocular Videos Using Temporal Consistency",
    "authors": ["Karen Martinez", "Andrew Johnson", "Peter Zhang"],
    "abstract": "Estimating depth from monocular videos is a fundamental challenge in computer vision with applications in autonomous driving, robotics, and augmented reality. We present a self-supervised learning approach that leverages temporal consistency across video frames to train depth estimation models without ground truth depth data. Our key innovation is a new temporal consistency loss that penalizes inconsistent depth predictions across consecutive frames while accounting for camera motion and dynamic objects. Extensive experiments on KITTI, Cityscapes, and our new RealWorld-Depth dataset demonstrate state-of-the-art performance, reducing depth error by 18% compared to previous self-supervised methods while maintaining real-time inference speed on mobile devices.",
    "categories": ["cs.CV", "cs.AI"],
    "subcategories": ["vision"],
    "published": "2023-03-24T10:17:32Z",
    "updated": "2023-03-28T14:05:43Z",
    "url": "https://arxiv.org/abs/2303.14158",
    "pdfUrl": "https://arxiv.org/pdf/2303.14158.pdf",
    "publishedYear": 2023
  },
  {
    "id": "2308.09124",
    "arxivId": "2308.09124",
    "title": "Vision-Language Models for Zero-Shot Visual Reasoning",
    "authors": ["Olivia Wilson", "Marco Chen", "Aisha Patel"],
    "abstract": "Visual reasoning—the ability to analyze and draw conclusions from visual input—remains challenging for AI systems, especially in novel scenarios. We introduce VL-Reason, a vision-language model specifically designed for zero-shot visual reasoning tasks. VL-Reason extends existing vision-language architectures with a novel cross-modal reasoning module that performs iterative refinement between visual and textual representations. This approach enables the model to decompose complex reasoning tasks into simpler steps with interpretable intermediate results. Evaluated on challenging benchmarks including GQA, CLEVR, and VCR, our model demonstrates superior zero-shot performance compared to existing approaches. Notably, VL-Reason shows strong performance on compositional reasoning tasks that require multi-step inference, even without any task-specific training examples.",
    "categories": ["cs.CV", "cs.CL", "cs.AI"],
    "subcategories": ["vision", "nlp"],
    "published": "2023-08-17T16:42:28Z",
    "updated": "2023-08-20T09:15:56Z",
    "url": "https://arxiv.org/abs/2308.09124",
    "pdfUrl": "https://arxiv.org/pdf/2308.09124.pdf",
    "publishedYear": 2023
  }
]