papers:
  - id: 2412.20504v5
    title: >-
      ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video  
      Understanding
    abstract: >-
      Video Large Language Models (VideoLLMs) have made significant strides in
      video understanding but struggle with long videos due to the limitations
      of their backbone LLMs. Existing solutions rely on length extrapolation,
      which is memory-constrained, or visual token compression, which primarily
      leverages low-level temporal redundancy while overlooking the more
      effective high-level knowledge redundancy. To address this, we propose
      $\textbf{ReTaKe}$, a training-free method with two novel modules DPSelect
      and PivotKV, to jointly reduce both temporal visual redundancy and
      knowledge redundancy for video compression. To align with the way of human
      temporal perception, DPSelect identifies keyframes based on inter-frame
      distance peaks. To leverage LLMs' learned prior knowledge, PivotKV marks
      the keyframes as pivots and compress non-pivot frames by pruning
      low-attention tokens in their KV cache. ReTaKe enables VideoLLMs to
      process 8 times longer frames (up to 2048), outperforming similar-sized
      models by 3-5% and even rivaling much larger ones on VideoMME, MLVU,
      LongVideoBench, and LVBench. Moreover, by overlapping compression
      operations with prefilling, ReTaKe introduces only ~10% prefilling latency
      overhead while reducing decoding latency by ~20%. Our code is available at
      https://github.com/SCZwangxiao/video-ReTaKe.
    authors:
      - Xiao Wang
      - Qingyi Si
      - Jianlong Wu
      - Shiyu Zhu
      - Li Cao
      - Liqiang Nie
    year: 2024
    url: https://arxiv.org/abs/2412.20504v5
    pdfUrl: https://arxiv.org/pdf/2412.20504v5.pdf
    category: ai
    subcategory: vision
    venue: arXiv
    match_opponents:
      - id: '202402.7318'
        title: Privacy-Preserving Quantum Computing in Education
        url: https://arxiv.org/abs/202402.7318
        score: 85
        similarity: '0.82'
      - id: '202403.6015'
        title: Privacy-Preserving Attention Mechanism in Social Media
        url: https://arxiv.org/abs/202403.6015
        score: 63
        similarity: '0.69'
  - id: 2501.00103v1
    title: 'LTX-Video: Realtime Video Latent Diffusion'
    abstract: >-
      We introduce LTX-Video, a transformer-based latent diffusion model that
      adopts a holistic approach to video generation by seamlessly integrating
      the responsibilities of the Video-VAE and the denoising transformer.
      Unlike existing methods, which treat these components as independent,
      LTX-Video aims to optimize their interaction for improved efficiency and
      quality. At its core is a carefully designed Video-VAE that achieves a
      high compression ratio of 1:192, with spatiotemporal downscaling of 32 x
      32 x 8 pixels per token, enabled by relocating the patchifying operation
      from the transformer's input to the VAE's input. Operating in this highly
      compressed latent space enables the transformer to efficiently perform
      full spatiotemporal self-attention, which is essential for generating
      high-resolution videos with temporal consistency. However, the high
      compression inherently limits the representation of fine details. To
      address this, our VAE decoder is tasked with both latent-to-pixel
      conversion and the final denoising step, producing the clean result
      directly in pixel space. This approach preserves the ability to generate
      fine details without incurring the runtime cost of a separate upsampling
      module. Our model supports diverse use cases, including text-to-video and
      image-to-video generation, with both capabilities trained simultaneously.
      It achieves faster-than-real-time generation, producing 5 seconds of 24
      fps video at 768x512 resolution in just 2 seconds on an Nvidia H100 GPU,
      outperforming all existing models of similar scale. The source code and
      pre-trained models are publicly available, setting a new benchmark for
      accessible and scalable video generation.
    authors:
      - Yoav HaCohen
      - Nisan Chiprut
      - Benny Brazowski
      - Daniel Shalem
      - Dudu Moshe
      - Eitan Richardson
      - Eran Levin
      - Guy Shiran
      - Nir Zabari
      - Ori Gordon
      - Poriya Panet
      - Sapir Weissbuch
      - Victor Kulikov
      - Yaki Bitterman
      - Zeev Melumian
      - Ofir Bibi
    year: 2024
    url: https://arxiv.org/abs/2501.00103v1
    pdfUrl: https://arxiv.org/pdf/2501.00103v1.pdf
    category: ai
    subcategory: vision
    venue: arXiv
    match_opponents:
      - id: '202412.5009'
        title: Adaptive Graph Neural Network for Dynamic Database Management
        url: https://arxiv.org/abs/202412.5009
        score: 15
        similarity: '0.68'
      - id: '202410.4186'
        title: Scalable Computer Vision Framework for Algorithm Design
        url: https://arxiv.org/abs/202410.4186
        score: 13
        similarity: '0.94'
      - id: '202404.8055'
        title: Optimizing Data Privacy Through Quantum Computing
        url: https://arxiv.org/abs/202404.8055
        score: 23
        similarity: '0.94'
      - id: '202411.8996'
        title: A Novel Approach to Cloud Computing Using Optimization
        url: https://arxiv.org/abs/202411.8996
        score: 91
        similarity: '0.80'
  - id: 2412.21206v1
    title: 'PERSE: Personalized 3D Generative Avatars from A Single Portrait'
    abstract: >-
      We present PERSE, a method for building an animatable personalized
      generative avatar from a reference portrait. Our avatar model enables
      facial attribute editing in a continuous and disentangled latent space to
      control each facial attribute, while preserving the individual's identity.
      To achieve this, our method begins by synthesizing large-scale synthetic
      2D video datasets, where each video contains consistent changes in the
      facial expression and viewpoint, combined with a variation in a specific
      facial attribute from the original input. We propose a novel pipeline to
      produce high-quality, photorealistic 2D videos with facial attribute
      editing. Leveraging this synthetic attribute dataset, we present a
      personalized avatar creation method based on the 3D Gaussian Splatting,
      learning a continuous and disentangled latent space for intuitive facial
      attribute manipulation. To enforce smooth transitions in this latent
      space, we introduce a latent space regularization technique by using
      interpolated 2D faces as supervision. Compared to previous approaches, we
      demonstrate that PERSE generates high-quality avatars with interpolated
      attributes while preserving identity of reference person.
    authors:
      - Hyunsoo Cha
      - Inhee Lee
      - Hanbyul Joo
    year: 2024
    url: https://arxiv.org/abs/2412.21206v1
    pdfUrl: https://arxiv.org/pdf/2412.21206v1.pdf
    category: ai
    subcategory: vision
    venue: arXiv
    match_opponents:
      - id: '202409.3801'
        title: Deep Learning Based Federated Learning for Resource Allocation
        url: https://arxiv.org/abs/202409.3801
        score: 60
        similarity: '0.81'
      - id: '202405.0700'
        title: Scalable Transformer Framework for Parallel Processing
        url: https://arxiv.org/abs/202405.0700
        score: 36
        similarity: '1.00'
      - id: '202405.4999'
        title: Distributed Neural Network Systems for Parallel Processing
        url: https://arxiv.org/abs/202405.4999
        score: 3
        similarity: '0.91'
  - id: 2501.00184v2
    title: 'TrajLearn: Trajectory Prediction Learning using Deep Generative Models'
    abstract: >-
      Trajectory prediction aims to estimate an entity's future path using its
      current position and historical movement data, benefiting fields like
      autonomous navigation, robotics, and human movement analytics. Deep
      learning approaches have become key in this area, utilizing large-scale
      trajectory datasets to model movement patterns, but face challenges in
      managing complex spatial dependencies and adapting to dynamic
      environments. To address these challenges, we introduce TrajLearn, a novel
      model for trajectory prediction that leverages generative modeling of
      higher-order mobility flows based on hexagonal spatial representation.
      TrajLearn predicts the next $k$ steps by integrating a customized beam
      search for exploring multiple potential paths while maintaining spatial
      continuity. We conducted a rigorous evaluation of TrajLearn, benchmarking
      it against leading state-of-the-art approaches and meaningful baselines.
      The results indicate that TrajLearn achieves significant performance
      gains, with improvements of up to ~40% across multiple real-world
      trajectory datasets. In addition, we evaluated different prediction
      horizons (i.e., various values of $k$), conducted resolution sensitivity
      analysis, and performed ablation studies to assess the impact of key model
      components. Furthermore, we developed a novel algorithm to generate
      mixed-resolution maps by hierarchically subdividing hexagonal regions into
      finer segments within a specified observation area. This approach supports
      selective detailing, applying finer resolution to areas of interest or
      high activity (e.g., urban centers) while using coarser resolution for
      less significant regions (e.g., rural areas), effectively reducing data
      storage requirements and computational overhead. We promote
      reproducibility and adaptability by offering complete code, data, and
      detailed documentation with flexible configuration options for various
      applications.
    authors:
      - Amirhossein Nadiri
      - Jing Li
      - Ali Faraji
      - Ghadeer Abuoda
      - Manos Papagelis
    year: 2024
    url: https://arxiv.org/abs/2501.00184v2
    pdfUrl: https://arxiv.org/pdf/2501.00184v2.pdf
    category: ai
    subcategory: vision
    venue: arXiv
    match_opponents:
      - id: '202406.4196'
        title: Deep Learning Based Cryptography for Cloud Computing
        url: https://arxiv.org/abs/202406.4196
        score: 99
        similarity: '0.75'
      - id: '202409.1855'
        title: >-
          Deep Learning Based Reinforcement Learning for Human-Computer
          Interaction
        url: https://arxiv.org/abs/202409.1855
        score: 49
        similarity: '0.62'
      - id: '202401.1360'
        title: A Survey of Distributed Systems in Smart Cities
        url: https://arxiv.org/abs/202401.1360
        score: 86
        similarity: '0.81'
      - id: '202410.9312'
        title: Privacy-Preserving Optimization in Finance
        url: https://arxiv.org/abs/202410.9312
        score: 15
        similarity: '0.92'
  - id: 2412.20830v1
    title: >-
      ReFlow6D: Refraction-Guided Transparent Object 6D Pose Estimation via  
      Intermediate Representation Learning
    abstract: >-
      Transparent objects are ubiquitous in daily life, making their perception
      and robotics manipulation important. However, they present a major
      challenge due to their distinct refractive and reflective properties when
      it comes to accurately estimating the 6D pose. To solve this, we present
      ReFlow6D, a novel method for transparent object 6D pose estimation that
      harnesses the refractive-intermediate representation. Unlike conventional
      approaches, our method leverages a feature space impervious to changes in
      RGB image space and independent of depth information. Drawing inspiration
      from image matting, we model the deformation of the light path through
      transparent objects, yielding a unique object-specific intermediate
      representation guided by light refraction that is independent of the
      environment in which objects are observed. By integrating these
      intermediate features into the pose estimation network, we show that
      ReFlow6D achieves precise 6D pose estimation of transparent objects, using
      only RGB images as input. Our method further introduces a novel
      transparent object compositing loss, fostering the generation of superior
      refractive-intermediate features. Empirical evaluations show that our
      approach significantly outperforms state-of-the-art methods on TOD and
      Trans32K-6D datasets. Robot grasping experiments further demonstrate that
      ReFlow6D's pose estimation accuracy effectively translates to real-world
      robotics task. The source code is available at:
      https://github.com/StoicGilgamesh/ReFlow6D and
      https://github.com/StoicGilgamesh/matting_rendering.
    authors:
      - Hrishikesh Gupta
      - Stefan Thalhammer
      - Jean-Baptiste Weibel
      - Alexander Haberl
      - Markus Vincze
    year: 2024
    url: https://arxiv.org/abs/2412.20830v1
    pdfUrl: https://arxiv.org/pdf/2412.20830v1.pdf
    category: ai
    subcategory: vision
    venue: arXiv
    match_opponents:
      - id: '202401.1993'
        title: Privacy-Preserving Transformer in Cybersecurity
        url: https://arxiv.org/abs/202401.1993
        score: 9
        similarity: '0.81'
      - id: '202409.9819'
        title: A Novel Approach to System Performance Using Edge Computing
        url: https://arxiv.org/abs/202409.9819
        score: 87
        similarity: '0.59'
      - id: '202410.6698'
        title: Optimizing Algorithm Design Through Graph Neural Network
        url: https://arxiv.org/abs/202410.6698
        score: 7
        similarity: '0.74'
  - id: 2412.20870v2
    title: 'SoftPatch+: Fully Unsupervised Anomaly Classification and Segmentation'
    abstract: >-
      Although mainstream unsupervised anomaly detection (AD) (including
      image-level classification and pixel-level segmentation)algorithms perform
      well in academic datasets, their performance is limited in practical
      application due to the ideal experimental setting of clean training data.
      Training with noisy data is an inevitable problem in real-world anomaly
      detection but is seldom discussed. This paper is the first to consider
      fully unsupervised industrial anomaly detection (i.e., unsupervised AD
      with noisy data). To solve this problem, we proposed memory-based
      unsupervised AD methods, SoftPatch and SoftPatch+, which efficiently
      denoise the data at the patch level. Noise discriminators are utilized to
      generate outlier scores for patch-level noise elimination before coreset
      construction. The scores are then stored in the memory bank to soften the
      anomaly detection boundary. Compared with existing methods, SoftPatch
      maintains a strong modeling ability of normal data and alleviates the
      overconfidence problem in coreset, and SoftPatch+ has more robust
      performance which is articularly useful in real-world industrial
      inspection scenarios with high levels of noise (from 10% to 40%).
      Comprehensive experiments conducted in diverse noise scenarios demonstrate
      that both SoftPatch and SoftPatch+ outperform the state-of-the-art AD
      methods on the MVTecAD, ViSA, and BTAD benchmarks. Furthermore, the
      performance of SoftPatch and SoftPatch+ is comparable to that of the
      noise-free methods in conventional unsupervised AD setting. The code of
      the proposed methods can be found at
      https://github.com/TencentYoutuResearch/AnomalyDetection-SoftPatch.
    authors:
      - Chengjie Wang
      - Xi Jiang
      - Bin-Bin Gao
      - Zhenye Gan
      - Yong Liu
      - Feng Zheng
      - Lizhuang Ma
    year: 2024
    url: https://arxiv.org/abs/2412.20870v2
    pdfUrl: https://arxiv.org/pdf/2412.20870v2.pdf
    category: ai
    subcategory: vision
    venue: arXiv
    match_opponents:
      - id: '202403.8855'
        title: Adaptive Machine Learning for Dynamic Robotics
        url: https://arxiv.org/abs/202403.8855
        score: 95
        similarity: '0.68'
      - id: '202409.5361'
        title: Privacy-Preserving Optimization in Transportation
        url: https://arxiv.org/abs/202409.5361
        score: 55
        similarity: '0.63'
