papers:
- abstract: Transformer models achieve state-of-the-art accuracy on natural language
    processing (NLP) and vision tasks, but demand significant computation and memory
    resources, which makes it difficult to perform inference and training (fine-tuning)
    on edge accelerators. Quantization to lower precision data types is a promising
    way to reduce computation and memory resources. Prior work has employed 8-bit
    integer (int8) quantization for Transformer inference, but int8 lacks the precision
    and range required for training. 8-bit floating-point (FP8) quantization has been
    used for Transformer training, but prior work only quantizes the inputs to matrix
    multiplications and leaves the rest of the operations in high precision.
  keywords: ''
  references: "Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods\
    \ for 8-bit training of neural networks, 2018. arXiv:1805. 11046. Tom B. Brown,\
    \ Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\
    \ Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\
    \ Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\
    \ Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\
    \ Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher\
    \ Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\
    \ models are few-shot learners, 2020. arXiv:2005.14165. Gian Carlo Cardarilli,\
    \ Luca Di Nunzio, Rocco Fazzolari, Daniele Giardino, Alberto Nannarelli, Marco\
    \ Re, and Sergio Span\xF2. A pseudo-softmax function for hardware-based high speed\
    \ image classification. Scientific Reports, 11(1):15307, 2021. Nicolas Carion,\
    \ Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\
    \ Sergey Zagoruyko. End-to-end object detection with transformers, 2020. arXiv:2005.12872.\
    \ Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised\
    \ vision transformers, 2021. arXiv:2104.02057. Marco Cococcioni, Federico Rossi,\
    \ Emanuele Ruffaldi, and Sergio Saponara. Fast approximations of activation functions\
    \ in deep neural networks when using posit arithmetic. Sensors, 20(5):1515, 2020.\
    \ Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8():\
    \ 8-bit matrix multiplication for transformers at scale, 2022. arXiv:2208.07339.\
    \ Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training\
    \ of deep bidirectional transformers for language understanding. In Proceedings\
    \ of the 2019 Conference of the North American Chapter of the Association for\
    \ Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\
    \ Papers), pages 4171--4186, Minneapolis, Minnesota, June 2019. Association for\
    \ Computational Linguistics. URL: https://aclanthology.org/N19-1423 Alexey Dosovitskiy,\
    \ Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\
    \ Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\
    \ and Neil Houlsby. An image is worth 16\xD716 words: Transformers for image recognition\
    \ at scale, 2021. arXiv:2010.11929. John Gustafson and Ivan Yonemoto. Beating\
    \ floating point at its own game: Posit arithmetic. Supercomput. Front. Innov.:\
    \ Int. J., 4(2):71--86, June 2017. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.\
    \ Distilling the knowledge in a neural network, 2015. arXiv:1503.02531. Neil Houlsby,\
    \ Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\
    \ Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer\
    \ learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings\
    \ of the 36th International Conference on Machine Learning, volume 97 of Proceedings\
    \ of Machine Learning Research, pages 2790--2799. PMLR, June 2019. URL: https://proceedings.mlr.press/v97/houlsby19a.html.\
    \ Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\
    \ Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language\
    \ models, 2021. arXiv:2106.09685. Jeff Johnson. Rethinking floating point for\
    \ deep learning, 2018. arXiv: 1811.01721. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei\
    \ Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin\
    \ Stoyanov. RoBERTa: A robustly optimized bert pretraining approach, 2019. arXiv:1907.11692.\
    \ Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and\
    \ Baining Guo. Swin Transformer: Hierarchical vision transformer using shifted\
    \ windows, 2021. arXiv:2103.14030. Jinming Lu, Chao Fang, Mingyang Xu, Jun Lin,\
    \ and Zhongfeng Wang. Evaluations on deep neural networks training using posit\
    \ number system. IEEE Transactions on Computers, 70(2):174--187, 2021. Paulius\
    \ Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David\
    \ Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh,\
    \ and Hao Wu. Mixed precision training, 2018. arXiv:1710.03740. Paulius Micikevicius,\
    \ Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite,\
    \ Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, Naveen Mellempudi,\
    \ Stuart Oberman, Mohammad Shoeybi, Michael Siu, and Hao Wu. FP8 formats for deep\
    \ learning, 2022. arXiv:2209.05433. Raul Murillo, Alberto A. Del Barrio, Guillermo\
    \ Botella, Min Soo Kim, HyunJin Kim, and Nader Bagherzadeh. PLAM: A posit logarithm-approximate\
    \ multiplier. IEEE Transactions on Emerging Topics in Computing, 10(4):2079--2085,\
    \ 2022. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech:\
    \ An ASR corpus based on public domain audio books. In 2015 IEEE International\
    \ Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5206--5210,\
    \ 2015. Kartik Prabhu, Albert Gural, Zainab F. Khan, Robert M. Radway, Massimo\
    \ Giordano, Kalhan Koul, Rohan Doshi, John W. Kustin, Timothy Liu, Gregorio B.\
    \ Lopes, Victor Turbiner, Win-San Khwa, Yu-Der Chih, Meng-Fan Chang, Gu\xE9nol\xE9\
    \ Lallement, Boris Murmann, Subhasish Mitra, and Priyanka Raina. CHIMERA: A 0.92-TOPS,\
    \ 2.2-TOPS/W edge AI accelerator with 2-MByte on-chip foundry resistive RAM for\
    \ efficient training and inference. IEEE Journal of Solid State Circuits, 57(4):1013--1026,\
    \ 2022. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey,\
    \ and Ilya Sutskever. Robust speech recognition via large-scale weak supervision,\
    \ 2022. arXiv:2212.04356. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\
    \ Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring\
    \ the limits of transfer learning with a unified text-to-text transformer. Journal\
    \ of Machine Learning Research, 21(140):1--67, 2020. URL: http://jmlr.org/papers/v21/20-074.html.\
    \ Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+\
    \ questions for machine comprehension of text. In Jian Su, Kevin Duh, and Xavier\
    \ Carreras, editors, Proceedings of the 2016 Conference on Empirical Methods in\
    \ Natural Language Processing, pages 2383--2392, Austin, Texas, November 2016.\
    \ Association for Computational Linguistics. URL: https://aclanthology.org/D16-1264\
    \ Gon\xE7alo Raposo, Pedro Tom\xE1s, and Nuno Roma. PositNN: Training deep neural\
    \ networks with mixed low-precision posit. In 2021 IEEE International Conference\
    \ on Acoustics, Speech and Signal Processing (ICASSP), pages 7908--7912, 2021.\
    \ Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter:\
    \ Transformer for semantic segmentation, 2021. arXiv: 2105.05633. Zhiqing Sun,\
    \ Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. MobileBERT:\
    \ A compact task-agnostic BERT for resource-limited devices. In Proceedings of\
    \ the 58th Annual Meeting of the Association for Computational Linguistics, pages\
    \ 2158--2170, Online, July 2020. Association for Computational Linguistics. URL:\
    \ https://aclanthology.org/2020.acl-main.195 Ashish Vaswani, Noam Shazeer, Niki\
    \ Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and\
    \ Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S.\
    \ Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances\
    \ in Neural Information Processing Systems, volume 30. Curran Associates, Inc.,\
    \ 2017. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and\
    \ Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural\
    \ language understanding. In Tal Linzen, Grzegorz Chrupa\u0142a, and Afra Alishahi,\
    \ editors, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\
    \ Neural Networks for NLP, pages 353--355, Brussels, Belgium, November 2018. Association\
    \ for Computational Linguistics. URL: https://aclanthology.org/W18-5446 Thomas\
    \ Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\
    \ Moi, Pierric Cistac, Tim Rault, R\xE9mi Louf, Morgan Funtowicz, Joe Davison,\
    \ Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen\
    \ Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander\
    \ M. Rush. HuggingFace's Transformers: State-of-the-art natural language processing,\
    \ 2020. arXiv:1910.03771. Haocheng Xi, Changhao Li, Jianfei Chen, and Jun Zhu.\
    \ Training transformers with 4-bit integers, 2023. arXiv:2306.11987. Guangxuan\
    \ Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant:\
    \ Accurate and efficient post-training quantization for large language models,\
    \ 2023. arXiv:2211.10438. Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg.\
    \ Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked\
    \ language-models, 2022. arXiv:2106.10199."
  title: 8-bit Transformer Inference and Fine-tuning for Edge Accelerators
  url: https://dl.acm.org/doi/10.1145/3620666.3651368
